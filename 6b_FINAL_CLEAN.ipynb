{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Final-Project-Check-in\" data-toc-modified-id=\"Final-Project-Check-in-1\">Final Project Check-in</a></span></li><li><span><a href=\"#Group-Name\" data-toc-modified-id=\"Group-Name-2\">Group Name</a></span></li><li><span><a href=\"#Student-Names\" data-toc-modified-id=\"Student-Names-3\">Student Names</a></span></li><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-4\">Load Data</a></span></li><li><span><a href=\"#Fit-scikit-learn-model\" data-toc-modified-id=\"Fit-scikit-learn-model-5\">Fit scikit-learn model</a></span></li><li><span><a href=\"#Evaluation-Metric\" data-toc-modified-id=\"Evaluation-Metric-6\">Evaluation Metric</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Project Check-in\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evan Chen\n",
    "----------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research Question / Hypothesis\n",
    "----\n",
    "\n",
    "## 1) Find the features that best predicts SalePrice of a house\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   category_encoders          import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from   sklearn.compose            import *\n",
    "from   sklearn.ensemble           import RandomForestRegressor, StackingRegressor, BaggingRegressor\n",
    "from   sklearn.experimental       import enable_iterative_imputer\n",
    "from   sklearn.impute             import *\n",
    "from   sklearn.linear_model       import *\n",
    "from   sklearn.metrics            import mean_squared_log_error, mean_squared_error, mean_absolute_error # We have not covered it yet in class. The basics - AUC is from 0 to 1 and higher is better.\n",
    "from   sklearn.pipeline           import Pipeline, FeatureUnion\n",
    "from   sklearn.preprocessing      import *\n",
    "from   sklearn.model_selection    import train_test_split\n",
    "from sklearn.model_selection      import cross_val_score\n",
    "from sklearn.inspection            import permutation_importance\n",
    "from sklearn.base               import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose            import make_column_transformer\n",
    "from sklearn.ensemble            import BaggingRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.model_selection    import RandomizedSearchCV\n",
    "from pandas_profiling import ProfileReport\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Evan/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:3188: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv('train.csv')\n",
    "y = df.SalePrice.values\n",
    "X = df.loc[:, df.columns!= 'SalePrice']\n",
    "\n",
    "# Change 'object' types to numeric\n",
    "X[['BsmtFullBath', 'FullBath', 'HalfBath', 'Fireplaces']] = X[['BsmtFullBath', 'FullBath', 'HalfBath', 'Fireplaces']].apply(pd.to_numeric)\n",
    "\n",
    "\n",
    "# Categorical Ordinal\n",
    "sale_cond = ['SaleCondition']\n",
    "sale_type = ['SaleType']\n",
    "qual_cat_ord = ['ExterQual', 'KitchenQual']\n",
    "functional_col = ['Functional']\n",
    "\n",
    "# Date Columns\n",
    "year_cols = ['YrSold', 'YearBuilt', 'YearRemodAdd']\n",
    "\n",
    "# OHE columns\n",
    "OHE_cols = ['CentralAir', 'Neighborhood']\n",
    "\n",
    "num_features =['LotArea', 'OverallQual', 'OverallCond',\n",
    "       'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n",
    "       'BsmtFullBath', 'FullBath', 'HalfBath', 'TotRmsAbvGrd', 'Fireplaces',\n",
    "       'GarageArea', 'WoodDeckSF', 'PoolArea', 'MiscVal', 'LotFrontage']\n",
    "\n",
    "# Change Dataframe to only contain features that we want\n",
    "num_features = num_features + functional_col + qual_cat_ord + year_cols + sale_cond + sale_type\n",
    "feature_cols = num_features + OHE_cols\n",
    "X = X[feature_cols]\n",
    "\n",
    "# Split to train/test\n",
    "X, X_test, y, y_test = train_test_split(X,y, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DateTime Encoding: YrSold, YearBuilt, YearRemod,  = categorical ordinal. The newest is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_dates(X):\n",
    "    return [2020- year for year in X.values] \n",
    "\n",
    "transformer_age = FunctionTransformer(map_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Transform for Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to turn into FunctionTransformer\n",
    "def map_SaleCond(X):\n",
    "    \"Replace value in array with the mapped value\"\n",
    "    map_dict = {'Normal' : 2,\n",
    "                'Abnorml': 0,\n",
    "                'Partial': 1,\n",
    "                'Alloca' : 1,\n",
    "                'AdjLand': 1,\n",
    "                'Family': 1}\n",
    "    return [map_dict[value] for value in X]\n",
    "\n",
    "def map_SaleType(X):\n",
    "    \"Replace value in array with the mapped value\"\n",
    "    map_dict = {'New' : 1,\n",
    "                'WD': 0,\n",
    "                'COD': 0,\n",
    "                'Con' : 0,\n",
    "                'ConLD': 0,\n",
    "                'ConLw': 0,\n",
    "                'CWD' : 0,\n",
    "                'ConLI': 0,\n",
    "               'Oth' : 0}\n",
    "\n",
    "    return [map_dict[value] for value in X]\n",
    "\n",
    "# Change functions to transformers\n",
    "transformer_SaleCond = FunctionTransformer(map_SaleCond)\n",
    "transformer_SaleType = FunctionTransformer(map_SaleType)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Ordinal Categorical and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fa=Fair, Gd=Good, TA=Typical, Ex=Excellent ranked in order\n",
    "quality_enc = OrdinalEncoder(categories=[['Fa', 'Gd', 'TA', 'Ex']])\n",
    "X['ExterQual'] = quality_enc.fit_transform(X[['ExterQual']])\n",
    "X['KitchenQual'] =  quality_enc.fit_transform(X[['KitchenQual']])\n",
    "\n",
    "# How well the house functions\n",
    "functional_enc = OrdinalEncoder(categories=[['Sev', 'Maj2', 'Mod', 'Min2', 'Maj1', 'Min1', 'Typ']])\n",
    "X['Functional'] = functional_enc.fit_transform(X[['Functional']])\n",
    "\n",
    "# Transform year columns to subtact from day\n",
    "for col in year_cols:\n",
    "    X[col] = transformer_age.fit_transform(X[col])\n",
    "\n",
    "# Transform SaleType and SaleCondition columns\n",
    "X['SaleType'] = transformer_SaleType.fit_transform(X['SaleType'].values)\n",
    "X['SaleCondition'] = transformer_SaleCond.fit_transform(X['SaleCondition'].values)\n",
    "\n",
    "# Numerical preprocessing\n",
    "num_pipe = Pipeline([('imputer', SimpleImputer(strategy='median')), #  add_indicator=True)),\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('QT', QuantileTransformer(n_quantiles=100, output_distribution='normal'))\n",
    "                    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL LinearRegression() AVG MAE: 21271.913428021842 RMSE_log: 0.13797731436524036 AVG MAPE: 13.04065380477186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Evan/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 50056357280.21478, tolerance: 498271033.57073164\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/Evan/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11176373051.332031, tolerance: 575653712.2288579\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/Evan/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 17470656284.795776, tolerance: 550014979.6167637\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/Evan/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7362121896.782593, tolerance: 591651051.0267992\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/Evan/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11296911247.578125, tolerance: 582734202.6179547\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/Evan/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9500781340.35913, tolerance: 599427005.6163993\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/Evan/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10611510378.86267, tolerance: 588746453.9630775\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/Evan/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10860587763.372314, tolerance: 539344080.2383778\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL Lasso() AVG MAE: 21853.738441568225 RMSE_log: 0.40777248235476576 AVG MAPE: 13.062634493497143\n",
      "MODEL LassoCV() AVG MAE: 22486.246652020833 RMSE_log: 0.1489549123889992 AVG MAPE: 13.302407974224526\n",
      "MODEL Ridge() AVG MAE: 20885.737116582215 RMSE_log: 0.12878704057362528 AVG MAPE: 12.158541760427742\n",
      "MODEL RidgeCV(alphas=array([ 0.1,  1. , 10. ])) AVG MAE: 21236.508135266027 RMSE_log: 0.13882966287597703 AVG MAPE: 13.044914244722738\n",
      "MODEL ElasticNet() AVG MAE: 24334.46037267516 RMSE_log: 0.38761613108331694 AVG MAPE: 14.870454275819549\n",
      "MODEL BayesianRidge() AVG MAE: 21652.497124101767 RMSE_log: 0.14592352679439266 AVG MAPE: 13.461921178699715\n",
      "MODEL GradientBoostingRegressor() AVG MAE: 17323.773054938414 RMSE_log: 0.1415705673849744 AVG MAPE: 9.968161215528053\n",
      "MODEL RandomForestRegressor() AVG MAE: 19387.949173789173 RMSE_log: 0.146613380275776 AVG MAPE: 11.090123879752879\n",
      "MODEL SGDRegressor() AVG MAE: 22487.020848440097 RMSE_log: 0.36253878729073985 AVG MAPE: 14.24770923564979\n",
      "MODEL AdaBoostRegressor() AVG MAE: 24561.18649083255 RMSE_log: 0.18690584813210653 AVG MAPE: 16.50770380781929\n"
     ]
    }
   ],
   "source": [
    "# MAPE Function\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# List of Regressors to try\n",
    "regressors = [LinearRegression(), Lasso(), LassoCV(), Ridge(), RidgeCV(),\n",
    "              ElasticNet(), BayesianRidge(), \n",
    "            GradientBoostingRegressor(), RandomForestRegressor(), SGDRegressor(), AdaBoostRegressor()]\n",
    "\n",
    "# Search regressors for lowest loss metric\n",
    "for reg in regressors: \n",
    "    \n",
    "    # Lists to calculate the average MAE and RMSE\n",
    "    MAE_avg = []\n",
    "    RMSE_avg = []\n",
    "    MAPE_avg = []\n",
    "    \n",
    "    # Preprocessor\n",
    "    prep = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('OHE', OneHotEncoder(handle_unknown='ignore'), OHE_cols),\n",
    "            ('num', num_pipe, num_features) ])\n",
    "    # Pipeline\n",
    "    pipe = Pipeline([('prep', prep),\n",
    "                     ('model', reg)])\n",
    "\n",
    "    # Iterate to get RMSEs and MAEs across different splits\n",
    "    for i in range(10): \n",
    "        X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.2)\n",
    "        y_train_log = np.log(y_train)\n",
    "        y_val_log = np.log(y_val)\n",
    "        \n",
    "        # Fit Model for MAE\n",
    "        model_1 = pipe.fit(X_train, y_train)\n",
    "        y_pred_1 = pipe.predict(X_val)\n",
    "        \n",
    "        # Fit Model for Log_RMSE\n",
    "        model_2 = pipe.fit(X_train, y_train_log)\n",
    "        y_pred_2 = pipe.predict(X_val)\n",
    "        \n",
    "        # Get Losses\n",
    "        mae = mean_absolute_error(y_val, y_pred_1)\n",
    "        mape = mean_absolute_percentage_error(y_val, y_pred_1)\n",
    "        rmse_log = np.sqrt(mean_squared_error(y_val_log, y_pred_2))\n",
    "        # Add losses to array\n",
    "        MAPE_avg.append(mape)\n",
    "        MAE_avg.append(mae)\n",
    "        RMSE_avg.append(rmse_log)\n",
    "    print(\"MODEL\", reg, \"AVG MAE:\", np.mean(MAE_avg), \"RMSE_log:\", np.mean(RMSE_avg), \"AVG MAPE:\", np.mean(MAPE_avg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regressor and RandomForest performs the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class\n",
    "class DummyEstimator(BaseEstimator):\n",
    "    \"Pass through class, methods are present but do nothing.\"\n",
    "    def fit(self): pass\n",
    "    def score(self): pass\n",
    "    \n",
    "# Placeholder for Searching Hyperparameters\n",
    "dummy_pipe = Pipeline([\n",
    "                      ('prep', prep),\n",
    "                    ('reg', DummyEstimator())]) # Placeholder Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Turning\n",
    "#### learning rate: must iteratively guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   31.6s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reg__validation_fraction': 0.2, 'reg__n_estimators': 200, 'reg__min_samples_split': 8, 'reg__min_samples_leaf': 5, 'reg__max_features': 'log2', 'reg__learning_rate': 0.1, 'reg__criterion': 'mse', 'reg': GradientBoostingRegressor(criterion='mse', max_features='log2',\n",
      "                          min_samples_leaf=5, min_samples_split=8,\n",
      "                          n_estimators=200, random_state=0,\n",
      "                          validation_fraction=0.2)}\n",
      "The validation set - 0.8780864321495413\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Hyperparameter tuning , set random state for consistency\n",
    "tree_hyp = dict(reg        = [RandomForestRegressor(random_state=0)],\n",
    "                reg__max_depth        = range(1, 10),\n",
    "                reg__max_features = ['log2', 'auto', 'sqrt'],\n",
    "               reg__min_samples_leaf = range(3, 10),\n",
    "               reg__min_samples_split = range(3, 10),\n",
    "                reg__n_estimators = [100,200,300])\n",
    "\n",
    "# Gradient Boosting Hyperparameter tuning,  set random state for consistency\n",
    "boost_hyp = dict(reg        = [GradientBoostingRegressor(random_state=0)],\n",
    "                 reg__learning_rate        = [0.0001, 0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2],\n",
    "               reg__criterion        = ['mse', 'friedman_mse'],\n",
    "                reg__max_features = ['log2', 'auto', 'sqrt'],\n",
    "               reg__min_samples_leaf = range(3, 10),\n",
    "               reg__min_samples_split = range(3, 10),\n",
    "                 reg__validation_fraction =  [0.1, 0.15, 0.2],\n",
    "                reg__n_estimators = [100,200,300])\n",
    "\n",
    "# Merge Search Space\n",
    "search_space = [tree_hyp, boost_hyp]\n",
    "\n",
    "# Ranodmized CV search\n",
    "clf_algos_rand = RandomizedSearchCV(estimator=dummy_pipe, \n",
    "                                    param_distributions=search_space, \n",
    "                                    n_iter=100,\n",
    "                                    cv=5, \n",
    "                                    n_jobs=-1,\n",
    "                                    verbose=1,\n",
    "                                   scoring='r2')\n",
    "# Get the best model \n",
    "result = clf_algos_rand.fit(X_train, y_train)\n",
    "best_model = result.best_estimator_\n",
    "\n",
    "#Get the best hyperparams\n",
    "tuned_hyperparams = result.best_params_\n",
    "print(tuned_hyperparams)\n",
    "print(f\"The validation set - {result.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reg__validation_fraction': 0.2,\n",
       " 'reg__n_estimators': 200,\n",
       " 'reg__min_samples_split': 8,\n",
       " 'reg__min_samples_leaf': 5,\n",
       " 'reg__max_features': 'log2',\n",
       " 'reg__learning_rate': 0.1,\n",
       " 'reg__criterion': 'mse',\n",
       " 'reg': GradientBoostingRegressor(criterion='mse', max_features='log2',\n",
       "                           min_samples_leaf=5, min_samples_split=8,\n",
       "                           n_estimators=200, random_state=0,\n",
       "                           validation_fraction=0.2)}"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default \n",
    "default = GradientBoostingRegressor().get_params()\n",
    "tuned = default\n",
    "\n",
    "# Assign tuned hyperparameter values\n",
    "tuned['validation_fraction'] = tuned_hyperparams['reg__validation_fraction']\n",
    "tuned['n_estimators'] = tuned_hyperparams['reg__n_estimators']\n",
    "tuned['min_samples_leaf'] = tuned_hyperparams['reg__min_samples_leaf']\n",
    "tuned['min_samples_split'] = tuned_hyperparams['reg__min_samples_split']\n",
    "tuned['max_features'] = tuned_hyperparams['reg__max_features']\n",
    "tuned['learning_rate'] = tuned_hyperparams['reg__learning_rate']\n",
    "tuned['criterion'] = tuned_hyperparams['reg__criterion']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST MAE: 18469.670914198967 Log RMSE: 0.1396350373650737 TEST MAPE: 9.62202567554539\n"
     ]
    }
   ],
   "source": [
    "# Pipeline\n",
    "pipe = Pipeline([('prep', prep),\n",
    "                 ('model', GradientBoostingRegressor(**tuned))])\n",
    "    \n",
    "# Log Transform\n",
    "y_train_log = np.log(y_train)\n",
    "y_val_log = np.log(y_val)\n",
    "\n",
    "# Fit Model for MAE\n",
    "model_1 = pipe.fit(X_train, y_train)\n",
    "y_pred_1 = pipe.predict(X_val)\n",
    "\n",
    "# Fit Model for Log_RMSE\n",
    "model_2 = pipe.fit(X_train, y_train_log)\n",
    "y_pred_2 = pipe.predict(X_val)\n",
    "\n",
    "# Get Losses\n",
    "mape = mean_absolute_percentage_error(y_val, y_pred_1)\n",
    "mae = mean_absolute_error(y_val, y_pred_1)\n",
    "rmse_log = np.sqrt(mean_squared_error(y_val_log, y_pred_2))\n",
    "print(\"TEST MAE:\", mae, \"Log RMSE:\", rmse_log, \"TEST MAPE:\", mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### MAE: 16,759.92 means that our predictions were on average 16,759 dollars away from the true house price in the test set. The MAPE shows that this 16759 is equal to a 10.04% error from the true house price. Log RMSE: 0.125 is the Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.\n",
    "\n",
    "\n",
    "# Lessons Learned\n",
    "### Features Selection: There could have been more time spent on feature selection by running permutation importances on all features preprocessing models; however, I had some difficulty getting the Column Transformers to work in the Pipeline. Due to the amount of categorical features with high cardinality, I had to carefully select the ones that I wanted to encode. It turns out that the \"Neighborhood\" column was very important when I ran a permutation importance test on my feature columns; however, it had 24 unique variables and encoding more would cause the curse of dimmensionality\n",
    "\n",
    "### Feature Engineering: In the future, more time could be spent on extracting more information from the unique variables for each categorical column. Perhaps PCA would be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
